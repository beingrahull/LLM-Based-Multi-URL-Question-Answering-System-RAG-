{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohIYaKO1ynDs"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-pinecone pinecone-client langchain-huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn-iZUyEOAe6"
      },
      "outputs": [],
      "source": [
        "pip install pinecone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WycNsdE2fLNG"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio langchain-pinecone pinecone-client langchain-huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adD1KFskKFhH"
      },
      "outputs": [],
      "source": [
        "!pip -q install langchain\n",
        "!pip -q install bitsandbytes accelerate transformers\n",
        "!pip -q install datasets sentencepiece peft\n",
        "!pip -q install pypdf\n",
        "!pip -q install sentence-transformers\n",
        "!pip -q install langchain-community\n",
        "!pip -q install langchain-text-splitters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2p3q1W8TMPU8"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qtcjJGluMcGk"
      },
      "outputs": [],
      "source": [
        "!pip install tokenizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OsPFWHAdMg6T"
      },
      "outputs": [],
      "source": [
        "#!pip install xformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VtW6Do6zNdVv"
      },
      "source": [
        "Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlmAzYFXTsKb"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-classic\n",
        "!pip install langchain-huggingface\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GhCoT-QYf6ou"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0HiAu8UNV6E"
      },
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import UnstructuredURLLoader\n",
        "from langchain_text_splitters import CharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain_community.vectorstores import Pinecone as LC_Pinecone\n",
        "from langchain_classic.chains import RetrievalQA\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import pipeline\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from huggingface_hub import notebook_login\n",
        "import textwrap\n",
        "import sys\n",
        "import os\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5dIDysoP4Y4"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"averaged_perceptron_tagger\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VytKHvH3Uf83"
      },
      "source": [
        "Passing URLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uiXIETM4Ui7j"
      },
      "outputs": [],
      "source": [
        "URL=['https://www.evolvingdev.com/post/what-is-llama-2',\n",
        "     'https://www.sciencedirect.com/science/article/pii/S266734522300024X',\n",
        "     'https://www.sciencenewstoday.org/what-is-gemini-ai-the-future-of-multimodal-artificial-intelligence-explained',\n",
        "     'https://www.cnet.com/tech/services-and-software/what-is-copilot-everything-you-need-to-know-about-microsofts-ai-tools/']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LJ-0au7U1fo"
      },
      "outputs": [],
      "source": [
        "loader=UnstructuredURLLoader(urls=URL)\n",
        "data=loader.load()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bnei_N4fVC_K"
      },
      "outputs": [],
      "source": [
        "data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PZgppthV8Mn"
      },
      "outputs": [],
      "source": [
        "len(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66lO_z8tWhAX"
      },
      "source": [
        "Splitting Extracted Data into Chunks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kg-c1KA_WkhW"
      },
      "outputs": [],
      "source": [
        "text_splitter=CharacterTextSplitter(separator='\\n',\n",
        "                                    chunk_size=1000,\n",
        "                                    chunk_overlap=200)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XJnqa2TsW5wA"
      },
      "outputs": [],
      "source": [
        "text_chunks=text_splitter.split_documents(data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tszMM8caW_ru"
      },
      "outputs": [],
      "source": [
        "len(text_chunks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx3EAP--XCLG"
      },
      "outputs": [],
      "source": [
        "text_chunks[9]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UE8371ZxXvj-"
      },
      "source": [
        "Downloading HuggingFaceEmbedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HidMKaOdXzVW"
      },
      "outputs": [],
      "source": [
        "embeddings=HuggingFaceEmbeddings()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbXdTdTPYAdd"
      },
      "outputs": [],
      "source": [
        "embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWie7UvlYXAo"
      },
      "source": [
        "Testing HF Embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wzVsWPVPYFFA"
      },
      "outputs": [],
      "source": [
        "a=embeddings.embed_query(\"Rahul\")\n",
        "len(a)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjMKy_tEYM6C"
      },
      "outputs": [],
      "source": [
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVGm6H6r1eVQ"
      },
      "source": [
        "384 re-establishment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjEn1aSy1eCo"
      },
      "outputs": [],
      "source": [
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "embeddings_384 = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
        ")\n",
        "\n",
        "print(len(embeddings_384.embed_query(\"check\")))  # must print 384\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZB7AyUjgYcxG"
      },
      "source": [
        "Converting Text Chunks into Embeddings and Creating a Knowledge Base"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BArK0541YcN9"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PINECONE_API_KEY='pcsk_9uK9o_82Vdb2t6w3CUMMbNB2DZTgnp7a2b2K9kBkuE2QZTeMwMJPrrHDJdTeZPW73FX7W'\n",
        "PINECONE_API_ENV='us-east-1'\n",
        "\n",
        "os.environ['PINECONE_API_KEY'] = PINECONE_API_KEY\n",
        "os.environ['PINECONE_ENVIRONMENT'] = PINECONE_API_ENV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFVDiqYROHMl"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "\n",
        "pc = Pinecone(api_key=\"pcsk_4NjoRg_5pzWfhnfsmnP2s9kJtpUf4MGVzhivWNHJCr3dqZP8fyo4AA3cLgo5y5tviGc23j\")\n",
        "index = pc.Index(\"llama-384\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MGjDIrQyOTcc"
      },
      "outputs": [],
      "source": [
        "pc.list_indexes()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T5e0AmmYbn6O"
      },
      "outputs": [],
      "source": [
        "index_name='llama'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tg1A4xp_2CSx"
      },
      "source": [
        "Establishing 384 Pinecone index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lObxNOon1r0R"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "# Initialize Pinecone client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# Create the index (only run once!)\n",
        "pc.create_index(\n",
        "    name=\"llama-384\",\n",
        "    dimension=384,\n",
        "    metric=\"cosine\",\n",
        "    spec=ServerlessSpec(\n",
        "        cloud=\"aws\",        # or \"gcp\"\n",
        "        region=\"us-east-1\"  # pick a valid region\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Index created successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l6qNMzGGzDm1"
      },
      "outputs": [],
      "source": [
        "from pinecone import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "\n",
        "PINECONE_API_KEY = \"pcsk_9uK9o_82Vdb2t6w3CUMMbNB2DZTgnp7a2b2K9kBkuE2QZTeMwMJPrrHDJdTeZPW73FX7W\"\n",
        "INDEX_NAME = \"llama\"\n",
        "\n",
        "# 1Ô∏è‚É£ Init Pinecone client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)\n",
        "\n",
        "# 2Ô∏è‚É£ Define embeddings FIRST\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L12-v2\"  # 768-dim\n",
        ")\n",
        "\n",
        "# 3Ô∏è‚É£ Connect to existing index (STRING name only)\n",
        "vectorstore = PineconeVectorStore.from_existing_index(\n",
        "    INDEX_NAME,\n",
        "    embeddings,\n",
        "    text_key=\"text\"\n",
        ")\n",
        "\n",
        "# 4Ô∏è‚É£ (Optional) Inspect index stats\n",
        "index = pc.Index(INDEX_NAME)\n",
        "print(\"‚úÖ VectorStore connected successfully!\")\n",
        "print(index.describe_index_stats())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkKLRet13gdK"
      },
      "source": [
        "Establishing new vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O7FNkbvg3gGh"
      },
      "outputs": [],
      "source": [
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "vectorstore_384 = PineconeVectorStore.from_existing_index(\n",
        "    \"llama-384\",\n",
        "    embeddings_384,\n",
        "    text_key=\"text\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxEGCWxd9OOP"
      },
      "outputs": [],
      "source": [
        "vectorstore_384.add_documents(text_chunks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pQxSZcM-S_M2"
      },
      "source": [
        "Creating LLM Wrapper"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gpAE6DEzVFMK"
      },
      "outputs": [],
      "source": [
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2prQE5QwVUtW"
      },
      "outputs": [],
      "source": [
        "model=\"daryl149/llama-2-7b-chat-hf\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "I88DJz60W-bE"
      },
      "outputs": [],
      "source": [
        "tokenizer=AutoTokenizer.from_pretrained(model,\n",
        "                                        use_auth_token=True,)\n",
        "model=AutoModelForCausalLM.from_pretrained(model,\n",
        "                                           device_map='auto',\n",
        "                                           torch_dtype=torch.float16,\n",
        "                                           use_auth_token=True,\n",
        "                                           load_in_8bit=True,\n",
        "                                           )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "956cnukGZLGQ"
      },
      "outputs": [],
      "source": [
        "pipe=pipeline(\"text-generation\",\n",
        "              model=model,\n",
        "              tokenizer=tokenizer,\n",
        "              torch_dtype=torch.bfloat16,\n",
        "              device_map=\"auto\",\n",
        "              max_new_tokens=512,\n",
        "              do_sample=True,\n",
        "              top_k=30,\n",
        "              num_return_sequences=1,\n",
        "              eos_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ev8JJLZ_c6vl"
      },
      "outputs": [],
      "source": [
        "llm=HuggingFacePipeline(pipeline=pipe, model_kwargs={'temperature':0.2})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "56FI77DHdXNz"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(\"Please provide a concise summary of ChatGPT and Gemini\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SLc8Q69IolYG"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(\"On what parameters does Iphone 13 differs from Iphone 15\")\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23rDyLrFpFc4"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.chains import RetrievalQA\n",
        "print(\"‚úÖ RetrievalQA imported successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZ3qhxVsa57F"
      },
      "outputs": [],
      "source": [
        "query=\"How was Chatgpt used for Students and Research Works\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfkRtk1mpW8j"
      },
      "outputs": [],
      "source": [
        "response = llm.invoke(query)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EEw-CU7yKbI"
      },
      "outputs": [],
      "source": [
        "print(len(vectorstore_384 ._embedding.embed_query(\"dimension check\")))\n",
        "\n",
        "doc=vectorstore_384 .similarity_search(query,k=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o5AHEycF48CQ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.documents import Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uq1lE-7q5Vu3"
      },
      "outputs": [],
      "source": [
        "print(type(doc[0]))\n",
        "print(doc[0].page_content)\n",
        "print(doc[0].metadata)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wshUB9eR4D8P"
      },
      "outputs": [],
      "source": [
        "doc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMJu5cNU1K43"
      },
      "outputs": [],
      "source": [
        "test_vec = embeddings.embed_query(\"test\")\n",
        "print(len(test_vec))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BoYaiH7m-Lqv"
      },
      "outputs": [],
      "source": [
        "qa=RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=vectorstore_384.as_retriever())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SnU4v5oU-wys"
      },
      "outputs": [],
      "source": [
        "question=\"What are capabilities of ChatGPT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l1MC0H47-34v"
      },
      "outputs": [],
      "source": [
        "qa.run(question)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yptxmbYkCAKA"
      },
      "source": [
        "Prompt Template"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RQxq6i-eEYLJ"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
        "Use ONLY the following context from {URL}: {context}\n",
        "\n",
        "Question: {input}\n",
        "\n",
        "If the context doesn't contain relevant information, respond exactly:\n",
        "\"I can only answer questions about {URL}\"\n",
        "\n",
        "Answer:\n",
        "\"\"\")\n",
        "\n",
        "# Rebuild your qa chain with strict prompt\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever,\n",
        "    chain_type_kwargs={\"prompt\": prompt}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L9FHQjX5EbMf"
      },
      "outputs": [],
      "source": [
        "from langchain_classic.chains import RetrievalQA\n",
        "\n",
        "# Create retriever\n",
        "retriever = vectorstore_384.as_retriever(search_kwargs={\"k\": 3})  # limit to top 3 docs\n",
        "\n",
        "# Setup QA without any memory\n",
        "qa = RetrievalQA.from_chain_type(\n",
        "    llm=llm,\n",
        "    chain_type=\"stuff\",\n",
        "    retriever=retriever\n",
        ")\n",
        "\n",
        "# Ask your question\n",
        "question = \"Who founded ChatGPT\"\n",
        "result = qa({\"query\": question})\n",
        "\n",
        "print(result['result'])  # Only uses retrieved docs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfM4IDhqlbK3"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "from gradio.components import Textbox\n",
        "\n",
        "def chat_with_url(question):\n",
        "    result = qa.invoke({\"query\": question})\n",
        "    return result[\"result\"]\n",
        "\n",
        "with gr.Blocks(title=\"URL Q&A Bot\") as demo:\n",
        "    gr.Markdown(\"# üöÄ URL Q&A Bot\")\n",
        "    gr.Markdown(f\"Query your Pinecone chunks from {URL}!\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=1):\n",
        "            question = Textbox(\n",
        "                label=\"Question\",\n",
        "                placeholder=f\"Ask about {URL}...\",\n",
        "                lines=3,\n",
        "                show_label=True\n",
        "            )\n",
        "        with gr.Column(scale=2):\n",
        "            answer = Textbox(\n",
        "                label=\"Answer\",\n",
        "                lines=10,\n",
        "                max_lines=20,\n",
        "                interactive=False  # Read-only answer box\n",
        "            )\n",
        "\n",
        "    # FIX: Clear question after submit + button backup\n",
        "    btn = gr.Button(\"Submit\", variant=\"primary\")\n",
        "    btn.click(chat_with_url, question, answer)\n",
        "    question.submit(chat_with_url, question, answer).then(\n",
        "        lambda: \"\", question, question  # Clear input after submit\n",
        "    )\n",
        "\n",
        "demo.launch(share=True)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}